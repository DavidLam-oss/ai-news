#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIæ—©æŠ¥çˆ¬è™«æœåŠ¡ä¸»ç¨‹åº
åŸºäºcrawl4aiå®ç°æ™ºèƒ½çˆ¬è™«ï¼ŒæŠ“å–AIç§‘æŠ€èµ„è®¯
"""

import asyncio
import json
import os
import sys
from datetime import datetime, timedelta
from typing import List, Dict, Any
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from crawl4ai import AsyncWebCrawler
from crawl4ai.extraction_strategy import LLMExtractionStrategy, LLMConfig
from loguru import logger
import schedule
import time
from dotenv import load_dotenv

from config.settings import Settings
from crawler.news_sources import NewsSources
from crawler.content_processor import ContentProcessor
import uvicorn

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class AINewsCrawler:
    """AIæ—©æŠ¥çˆ¬è™«ä¸»ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–çˆ¬è™«"""
        self.settings = Settings()
        self.news_sources = NewsSources()
        self.content_processor = ContentProcessor()
        self.crawler = None
        
        # é…ç½®æ—¥å¿—
        logger.add(
            "logs/crawler.log",
            rotation="1 day",
            retention="30 days",
            level=self.settings.LOG_LEVEL
        )
        
    async def init_crawler(self):
        """åˆå§‹åŒ–çˆ¬è™«å¼•æ“"""
        try:
            self.crawler = AsyncWebCrawler(
                headless=True,
                browser_type="chromium",
                user_agent=self.settings.USER_AGENT
            )
            await self.crawler.start()
            logger.info("çˆ¬è™«å¼•æ“åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"çˆ¬è™«å¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def crawl_news_sources(self) -> List[Dict[str, Any]]:
        """çˆ¬å–æ‰€æœ‰æ–°é—»æº"""
        all_articles = []
        
        # ä½¿ç”¨ç®€åŒ–çš„çˆ¬å–ç­–ç•¥ï¼Œé¿å…LLMæå–çš„å¤æ‚æ€§
        for source in self.news_sources.get_sources()[:3]:  # å…ˆæµ‹è¯•å‰3ä¸ªæº
            try:
                logger.info(f"å¼€å§‹çˆ¬å–: {source['name']}")
                
                # æ‰§è¡Œç®€å•çˆ¬å–
                result = await self.crawler.arun(
                    url=source['url'],
                    wait_for="networkidle",
                    delay_before_return_html=3,
                    js_code="""
                    // ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
                    await new Promise(resolve => setTimeout(resolve, 2000));
                    
                    // å°è¯•æ»šåŠ¨é¡µé¢ä»¥è§¦å‘æ‡’åŠ è½½
                    window.scrollTo(0, document.body.scrollHeight);
                    await new Promise(resolve => setTimeout(resolve, 1500));
                    window.scrollTo(0, 0);
                    """
                )
                
                if result.success:
                    # ä½¿ç”¨æˆ‘ä»¬æˆåŠŸçš„æ–‡ç« æå–é€»è¾‘
                    from improved_crawler_test import extract_articles_improved
                    articles = extract_articles_improved(result.html, source['name'], source['url'])
                    
                    # æ·»åŠ æ¥æºä¿¡æ¯
                    for article in articles:
                        article['source_url'] = source['url']
                        article['crawl_time'] = datetime.now().isoformat()
                    
                    all_articles.extend(articles)
                    logger.info(f"ä» {source['name']} è·å–åˆ° {len(articles)} ç¯‡æ–‡ç« ")
                else:
                    logger.warning(f"çˆ¬å– {source['name']} å¤±è´¥")
                    
            except Exception as e:
                logger.error(f"çˆ¬å– {source['name']} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                continue
        
        # å»é‡å’Œæ’åº
        unique_articles = self._deduplicate_articles(all_articles)
        sorted_articles = sorted(
            unique_articles, 
            key=lambda x: x.get('publish_time', ''), 
            reverse=True
        )
        
        logger.info(f"æ€»å…±è·å–åˆ° {len(sorted_articles)} ç¯‡å»é‡åçš„æ–‡ç« ")
        return sorted_articles[:self.settings.MAX_ARTICLES]
    
    def _deduplicate_articles(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ–‡ç« å»é‡"""
        seen_urls = set()
        unique_articles = []
        
        for article in articles:
            url = article.get('url', '')
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_articles.append(article)
        
        return unique_articles
    
    async def generate_daily_report(self, articles: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆæ¯æ—¥æ—©æŠ¥"""
        try:
            # ä½¿ç”¨AIå¤„ç†å†…å®¹
            processed_content = await self.content_processor.process_articles(articles)
            
            # ç”Ÿæˆæ—©æŠ¥
            report = {
                'date': datetime.now().strftime('%Y-%m-%d'),
                'title': f"AIç§‘æŠ€æ—©æŠ¥ - {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}",
                'summary': processed_content['summary'],
                'articles': processed_content['articles'],
                'trends': processed_content['trends'],
                'image_prompts': processed_content['image_prompts'],
                'created_at': datetime.now().isoformat()
            }
            
            logger.info("æ¯æ—¥æ—©æŠ¥ç”ŸæˆæˆåŠŸ")
            return report
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ¯æ—¥æ—©æŠ¥å¤±è´¥: {e}")
            raise
    
    async def save_to_feishu(self, report: Dict[str, Any]) -> bool:
        """ä¿å­˜åˆ°é£ä¹¦å¤šç»´è¡¨æ ¼"""
        try:
            from feishu.client import FeishuClient
            
            client = FeishuClient()
            
            # å‡†å¤‡æ•°æ® - ä½¿ç”¨æ­£ç¡®çš„å­—æ®µåç§°å’Œæ—¥æœŸæ ¼å¼
            from datetime import datetime
            current_date = datetime.now()
            
            record_data = {
                'æ—¥æœŸ': int(current_date.timestamp() * 1000),  # ä½¿ç”¨æ—¶é—´æˆ³æ ¼å¼
                'æ—©æŠ¥åŸå§‹å†…å®¹': json.dumps(report, ensure_ascii=False, indent=2),
                'å›¾ç‰‡æç¤ºè¯1': report['image_prompts'][0] if len(report['image_prompts']) > 0 else '',
                'å›¾ç‰‡æç¤ºè¯2': report['image_prompts'][1] if len(report['image_prompts']) > 1 else '',
                'å›¾ç‰‡æç¤ºè¯3': report['image_prompts'][2] if len(report['image_prompts']) > 2 else ''
            }
            
            # åˆ›å»ºè®°å½•
            success = await client.create_record(record_data)
            
            if success:
                logger.info("æ—©æŠ¥å·²ä¿å­˜åˆ°é£ä¹¦å¤šç»´è¡¨æ ¼")
            else:
                logger.error("ä¿å­˜åˆ°é£ä¹¦å¤šç»´è¡¨æ ¼å¤±è´¥")
                
            return success
            
        except Exception as e:
            logger.error(f"ä¿å­˜åˆ°é£ä¹¦å¤šç»´è¡¨æ ¼æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return False
    
    async def run_daily_crawl(self):
        """æ‰§è¡Œæ¯æ—¥çˆ¬å–ä»»åŠ¡"""
        try:
            logger.info("å¼€å§‹æ‰§è¡Œæ¯æ—¥çˆ¬å–ä»»åŠ¡")
            
            # çˆ¬å–æ–°é—»
            articles = await self.crawl_news_sources()
            
            if not articles:
                logger.warning("æœªè·å–åˆ°ä»»ä½•æ–‡ç« ï¼Œè·³è¿‡æ—©æŠ¥ç”Ÿæˆ")
                return
            
            # ç”Ÿæˆæ—©æŠ¥
            report = await self.generate_daily_report(articles)
            
            # ä¿å­˜åˆ°é£ä¹¦
            await self.save_to_feishu(report)
            
            # å‘é€åˆ°å¾®ä¿¡ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if self.settings.ENABLE_WECHAT:
                await self.send_to_wechat(report)
            
            logger.info("æ¯æ—¥çˆ¬å–ä»»åŠ¡å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ‰§è¡Œæ¯æ—¥çˆ¬å–ä»»åŠ¡å¤±è´¥: {e}")
    
    async def send_to_wechat(self, report: Dict[str, Any]):
        """å‘é€åˆ°å¾®ä¿¡"""
        try:
            # ä¼˜å…ˆä½¿ç”¨iPadåè®®å¾®ä¿¡åŠ©æ‰‹
            if self.settings.IPAD_WEBHOOK_URL:
                await self.send_to_wechat_ipad(report)
            else:
                # å›é€€åˆ°ä¼ ç»Ÿå¾®ä¿¡API
                await self.send_to_wechat_traditional(report)
            
        except Exception as e:
            logger.error(f"å‘é€åˆ°å¾®ä¿¡å¤±è´¥: {e}")
    
    async def send_to_wechat_ipad(self, report: Dict[str, Any]):
        """ä½¿ç”¨iPadåè®®å‘é€åˆ°å¾®ä¿¡"""
        try:
            from wechat.ipad_client import IpadWechatClient
            
            client = IpadWechatClient()
            
            # è¿æ¥
            if not await client.connect():
                logger.error("iPadå¾®ä¿¡å®¢æˆ·ç«¯è¿æ¥å¤±è´¥")
                return
            
            # å‘é€åˆ°é»˜è®¤ç¾¤
            success = await client.send_to_group(report)
            
            if success:
                logger.info("æ—©æŠ¥å·²é€šè¿‡iPadåè®®å‘é€åˆ°å¾®ä¿¡ç¾¤")
                
                # å¦‚æœé…ç½®äº†å¤šä¸ªç›®æ ‡ç¾¤ï¼Œå‘é€åˆ°æ‰€æœ‰ç¾¤
                if self.settings.TARGET_GROUPS:
                    group_names = [name.strip() for name in self.settings.TARGET_GROUPS.split(',')]
                    results = await client.send_to_multiple_groups(report, group_names)
                    
                    success_count = sum(1 for result in results.values() if result)
                    logger.info(f"æ—©æŠ¥å·²å‘é€åˆ° {success_count}/{len(group_names)} ä¸ªç¾¤")
                
                # å‘å¸ƒæœ‹å‹åœˆ
                moment_content = f"ğŸ¤– AIç§‘æŠ€æ—©æŠ¥ - {report['date']}\n\n{report['summary'][:200]}..."
                await client.publish_moment(moment_content)
                
            else:
                logger.error("é€šè¿‡iPadåè®®å‘é€åˆ°å¾®ä¿¡ç¾¤å¤±è´¥")
            
            await client.close()
            
        except Exception as e:
            logger.error(f"iPadåè®®å¾®ä¿¡å‘é€å¤±è´¥: {e}")
    
    async def send_to_wechat_traditional(self, report: Dict[str, Any]):
        """ä½¿ç”¨ä¼ ç»Ÿå¾®ä¿¡APIå‘é€"""
        try:
            from wechat.client import WechatClient
            
            client = WechatClient()
            
            # å‘é€åˆ°ç¾¤èŠ
            await client.send_to_group(report)
            
            # å‘å¸ƒæœ‹å‹åœˆ
            await client.publish_moment(report)
            
            logger.info("æ—©æŠ¥å·²é€šè¿‡ä¼ ç»ŸAPIå‘é€åˆ°å¾®ä¿¡")
            
        except Exception as e:
            logger.error(f"ä¼ ç»Ÿå¾®ä¿¡APIå‘é€å¤±è´¥: {e}")
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.crawler:
            await self.crawler.close()
            logger.info("çˆ¬è™«å¼•æ“å·²å…³é—­")

def run_scheduler():
    """è¿è¡Œå®šæ—¶ä»»åŠ¡"""
    crawler = AINewsCrawler()
    
    # è®¾ç½®å®šæ—¶ä»»åŠ¡ - æ¯å¤©ä¸Šåˆ8ç‚¹æ‰§è¡Œ
    schedule.every().day.at("08:00").do(
        lambda: asyncio.run(crawler.run_daily_crawl())
    )
    
    # ä¹Ÿå¯ä»¥è®¾ç½®æ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
    if os.getenv('DEBUG', 'False').lower() == 'true':
        schedule.every().hour.do(
            lambda: asyncio.run(crawler.run_daily_crawl())
        )
    
    logger.info("å®šæ—¶ä»»åŠ¡å·²å¯åŠ¨ï¼Œæ¯å¤©ä¸Šåˆ8ç‚¹æ‰§è¡Œçˆ¬å–")
    
    while True:
        schedule.run_pending()
        time.sleep(60)

async def main():
    """ä¸»å‡½æ•°"""
    crawler = AINewsCrawler()
    
    try:
        # åˆå§‹åŒ–çˆ¬è™«
        await crawler.init_crawler()
        
        # æ‰§è¡Œä¸€æ¬¡çˆ¬å–ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        await crawler.run_daily_crawl()
        
    except KeyboardInterrupt:
        logger.info("æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨å…³é—­...")
    except Exception as e:
        logger.error(f"ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
    finally:
        await crawler.cleanup()

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="AIæ—©æŠ¥çˆ¬è™«æœåŠ¡")
    parser.add_argument("--mode", choices=["once", "schedule", "api"], 
                       default="once", help="è¿è¡Œæ¨¡å¼")
    parser.add_argument("--host", default="0.0.0.0", help="APIæœåŠ¡ä¸»æœº")
    parser.add_argument("--port", type=int, default=8000, help="APIæœåŠ¡ç«¯å£")
    
    args = parser.parse_args()
    
    if args.mode == "once":
        # æ‰§è¡Œä¸€æ¬¡çˆ¬å–
        asyncio.run(main())
    elif args.mode == "schedule":
        # è¿è¡Œå®šæ—¶ä»»åŠ¡
        run_scheduler()
    elif args.mode == "api":
        # å¯åŠ¨APIæœåŠ¡
        uvicorn.run(app, host=args.host, port=args.port)
