#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¯æ—¥è‡ªåŠ¨çˆ¬è™«è„šæœ¬
æ¯å¤©å®šæ—¶æ‰§è¡Œï¼Œç”ŸæˆAIæ—©æŠ¥
"""

import asyncio
import sys
from pathlib import Path
from datetime import datetime
import schedule
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent))

from crawler.main import AINewsCrawler
from crawler.content_processor import ContentProcessor

async def daily_crawl_task():
    """æ¯æ—¥çˆ¬è™«ä»»åŠ¡"""
    
    print(f"ğŸ•·ï¸ å¼€å§‹æ‰§è¡Œæ¯æ—¥çˆ¬è™«ä»»åŠ¡ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = AINewsCrawler()
    
    try:
        # 1. åˆå§‹åŒ–çˆ¬è™«
        await crawler.init_crawler()
        print("âœ… çˆ¬è™«å¼•æ“åˆå§‹åŒ–æˆåŠŸ")
        
        # 2. çˆ¬å–æ–°é—»
        print("\nğŸ“° å¼€å§‹çˆ¬å–æ–°é—»...")
        articles = await crawler.crawl_news_sources()
        print(f"âœ… çˆ¬å–å®Œæˆï¼Œè·å–åˆ° {len(articles)} ç¯‡æ–‡ç« ")
        
        if not articles:
            print("âŒ æ²¡æœ‰è·å–åˆ°æ–‡ç« ï¼Œä»»åŠ¡ç»“æŸ")
            return
        
        # 3. AIå†…å®¹å¤„ç†
        print("\nğŸ¤– å¼€å§‹AIå†…å®¹å¤„ç†...")
        processor = ContentProcessor()
        processed_result = await processor.process_articles(articles[:10])  # å¤„ç†å‰10ç¯‡
        
        print("âœ… AIå¤„ç†å®Œæˆï¼")
        print(f"ğŸ“‹ æ—©æŠ¥æ‘˜è¦: {processed_result['summary'][:100]}...")
        
        # 4. ä¿å­˜åˆ°é£ä¹¦ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
        print("\nğŸ“Š ä¿å­˜åˆ°é£ä¹¦å¤šç»´è¡¨æ ¼...")
        try:
            record_data = {
                'æ—¥æœŸ': datetime.now().strftime('%Y-%m-%d'),
                'æ—©æŠ¥åŸå§‹å†…å®¹': f"çˆ¬å–åˆ°{len(articles)}ç¯‡æ–‡ç« ",
                'AIå¤„ç†åå†…å®¹': processed_result['summary'],
                'å›¾ç‰‡æç¤ºè¯1': processed_result['image_prompts'][0] if processed_result['image_prompts'] else '',
                'å›¾ç‰‡æç¤ºè¯2': processed_result['image_prompts'][1] if len(processed_result['image_prompts']) > 1 else '',
                'å›¾ç‰‡æç¤ºè¯3': processed_result['image_prompts'][2] if len(processed_result['image_prompts']) > 2 else ''
            }
            
            success = await crawler.save_to_feishu(record_data)
            if success:
                print("âœ… é£ä¹¦å­˜å‚¨æˆåŠŸï¼")
            else:
                print("âš ï¸ é£ä¹¦å­˜å‚¨å¤±è´¥ï¼ˆå¯èƒ½æœªé…ç½®ï¼‰")
                
        except Exception as e:
            print(f"âš ï¸ é£ä¹¦å­˜å‚¨å¤±è´¥: {e}")
        
        # 5. å¾®ä¿¡å‘é€åŠŸèƒ½å·²ä¸‹çº¿ï¼ˆé£é™©è§„é¿ï¼‰
        
        print(f"\nğŸ‰ æ¯æ—¥çˆ¬è™«ä»»åŠ¡å®Œæˆ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
    except Exception as e:
        print(f"âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # æ¸…ç†èµ„æº
        await crawler.cleanup()
        print("ğŸ§¹ èµ„æºæ¸…ç†å®Œæˆ")

def run_daily_crawl():
    """è¿è¡Œæ¯æ—¥çˆ¬è™«ä»»åŠ¡"""
    asyncio.run(daily_crawl_task())

def setup_schedule():
    """è®¾ç½®å®šæ—¶ä»»åŠ¡"""
    print("â° è®¾ç½®å®šæ—¶ä»»åŠ¡...")
    
    # æ¯å¤©ä¸Šåˆ8ç‚¹æ‰§è¡Œ
    schedule.every().day.at("08:00").do(run_daily_crawl)
    
    # æ¯å¤©ä¸‹åˆ6ç‚¹æ‰§è¡Œï¼ˆå¯é€‰ï¼‰
    # schedule.every().day.at("18:00").do(run_daily_crawl)
    
    print("âœ… å®šæ—¶ä»»åŠ¡è®¾ç½®å®Œæˆ")
    print("ğŸ“… æ‰§è¡Œæ—¶é—´: æ¯å¤©ä¸Šåˆ8:00")
    print("ğŸ”„ å¼€å§‹ç›‘æ§å®šæ—¶ä»»åŠ¡...")
    
    while True:
        schedule.run_pending()
        time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='AIæ—©æŠ¥ç³»ç»Ÿå®šæ—¶ä»»åŠ¡')
    parser.add_argument('--mode', choices=['once', 'schedule'], default='once',
                       help='è¿è¡Œæ¨¡å¼: once=æ‰§è¡Œä¸€æ¬¡, schedule=å®šæ—¶æ‰§è¡Œ')
    
    args = parser.parse_args()
    
    if args.mode == 'once':
        print("ğŸš€ æ‰§è¡Œå•æ¬¡çˆ¬è™«ä»»åŠ¡...")
        run_daily_crawl()
    else:
        print("ğŸš€ å¯åŠ¨å®šæ—¶ä»»åŠ¡æœåŠ¡...")
        setup_schedule()


