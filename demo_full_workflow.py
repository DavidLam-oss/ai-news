#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´å·¥ä½œæµç¨‹æ¼”ç¤º - çˆ¬è™« + AIå¤„ç† + é£ä¹¦å†™å…¥æ¨¡æ‹Ÿ
"""

import asyncio
import json
import os
import sys
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent))

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["FEISHU_APP_ID"] = "cli_a8366b7ef13a100c"
os.environ["FEISHU_APP_SECRET"] = "5nkWuj9xfU5bjg0qEJBcKhmX1H1ptjvr"
os.environ["FEISHU_BASE_URL"] = "https://open.feishu.cn/open-apis"
os.environ["FEISHU_TABLE_TOKEN"] = "F5I2bdNZxawzTqsRBVbcJWEMn9H"

from crawler.main import AINewsCrawler
from crawler.content_processor import ContentProcessor

async def demo_full_workflow():
    """æ¼”ç¤ºå®Œæ•´å·¥ä½œæµç¨‹"""
    print("ğŸš€ AIæ—©æŠ¥ç³»ç»Ÿå®Œæ•´å·¥ä½œæµç¨‹æ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = AINewsCrawler()
    
    try:
        # ç¬¬ä¸€æ­¥ï¼šåˆå§‹åŒ–çˆ¬è™«
        print("\nğŸ“¡ ç¬¬ä¸€æ­¥ï¼šåˆå§‹åŒ–çˆ¬è™«å¼•æ“...")
        await crawler.init_crawler()
        print("âœ… çˆ¬è™«å¼•æ“åˆå§‹åŒ–æˆåŠŸ")
        
        # ç¬¬äºŒæ­¥ï¼šçˆ¬å–æ–°é—»
        print("\nğŸ•·ï¸ ç¬¬äºŒæ­¥ï¼šçˆ¬å–AIç§‘æŠ€æ–°é—»...")
        articles = await crawler.crawl_news_sources()
        
        if articles:
            print(f"âœ… æˆåŠŸçˆ¬å–åˆ° {len(articles)} ç¯‡æ–‡ç« ")
            
            # æ˜¾ç¤ºå‰3ç¯‡æ–‡ç« 
            print("\nğŸ“° çˆ¬å–åˆ°çš„æ–‡ç« é¢„è§ˆï¼š")
            for i, article in enumerate(articles[:3], 1):
                print(f"  {i}. {article['title'][:60]}...")
                print(f"     æ¥æº: {article['source']}")
                print(f"     æ‘˜è¦: {article['summary'][:80]}...")
                print()
        else:
            print("âŒ æœªè·å–åˆ°ä»»ä½•æ–‡ç« ")
            return
        
        # ç¬¬ä¸‰æ­¥ï¼šAIå¤„ç†å†…å®¹
        print("\nğŸ¤– ç¬¬ä¸‰æ­¥ï¼šAIå¤„ç†å†…å®¹...")
        report = await crawler.generate_daily_report(articles)
        
        print("âœ… AIå†…å®¹å¤„ç†å®Œæˆ")
        print(f"ğŸ“… æ—©æŠ¥æ—¥æœŸ: {report['date']}")
        print(f"ğŸ“ æ—©æŠ¥æ ‡é¢˜: {report['title']}")
        
        # æ˜¾ç¤ºå¤„ç†ç»“æœ
        print("\nğŸ“‹ æ—©æŠ¥æ‘˜è¦:")
        print("-" * 50)
        print(report['summary'])
        
        print("\nğŸ“ˆ å‘å±•è¶‹åŠ¿:")
        print("-" * 50)
        for i, trend in enumerate(report['trends'][:5], 1):
            print(f"{i}. {trend}")
        
        print("\nğŸ¨ å›¾ç‰‡æç¤ºè¯:")
        print("-" * 50)
        for i, prompt in enumerate(report['image_prompts'], 1):
            print(f"{i}. {prompt}")
        
        # ç¬¬å››æ­¥ï¼šæ¨¡æ‹Ÿé£ä¹¦å†™å…¥
        print("\nğŸ’¾ ç¬¬å››æ­¥ï¼šå‡†å¤‡å†™å…¥é£ä¹¦å¤šç»´è¡¨æ ¼...")
        
        # å‡†å¤‡é£ä¹¦æ•°æ®æ ¼å¼
        feishu_data = {
            'æ—¥æœŸ': report['date'],
            'æ—©æŠ¥åŸå§‹å†…å®¹': json.dumps(report, ensure_ascii=False, indent=2),
            'AIå¤„ç†åå†…å®¹': report['summary'],
            'å›¾ç‰‡æç¤ºè¯1': report['image_prompts'][0] if len(report['image_prompts']) > 0 else '',
            'å›¾ç‰‡æç¤ºè¯2': report['image_prompts'][1] if len(report['image_prompts']) > 1 else '',
            'å›¾ç‰‡æç¤ºè¯3': report['image_prompts'][2] if len(report['image_prompts']) > 2 else ''
        }
        
        print("âœ… æ•°æ®æ ¼å¼å‡†å¤‡å®Œæˆ")
        print(f"ğŸ“Š æ•°æ®å­—æ®µæ•°é‡: {len(feishu_data)}")
        
        # å°è¯•å†™å…¥é£ä¹¦ï¼ˆå¯èƒ½ä¼šå¤±è´¥ï¼Œä½†å±•ç¤ºæµç¨‹ï¼‰
        print("\nğŸ”— ç¬¬äº”æ­¥ï¼šå°è¯•å†™å…¥é£ä¹¦å¤šç»´è¡¨æ ¼...")
        try:
            success = await crawler.save_to_feishu(report)
            if success:
                print("âœ… æ•°æ®æˆåŠŸå†™å…¥é£ä¹¦å¤šç»´è¡¨æ ¼ï¼")
            else:
                print("âš ï¸ é£ä¹¦å†™å…¥å¤±è´¥ï¼ˆå¯èƒ½æ˜¯é…ç½®é—®é¢˜ï¼‰")
                print("ğŸ’¡ è¿™æ˜¯æ­£å¸¸çš„ï¼Œå› ä¸ºéœ€è¦æ­£ç¡®çš„è¡¨æ ¼tokenå’Œæƒé™")
        except Exception as e:
            print(f"âš ï¸ é£ä¹¦å†™å…¥è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            print("ğŸ’¡ è¿™æ˜¯æ­£å¸¸çš„ï¼Œå› ä¸ºéœ€è¦æ­£ç¡®çš„è¡¨æ ¼tokenå’Œæƒé™")
        
        # æ˜¾ç¤ºå®Œæ•´çš„æ•°æ®ç»“æ„
        print("\nğŸ“‹ å®Œæ•´çš„æ•°æ®ç»“æ„é¢„è§ˆï¼š")
        print("-" * 50)
        print(json.dumps(feishu_data, ensure_ascii=False, indent=2)[:500] + "...")
        
        print("\nğŸ‰ å·¥ä½œæµç¨‹æ¼”ç¤ºå®Œæˆï¼")
        print("\nğŸ“ æ€»ç»“ï¼š")
        print("âœ… çˆ¬è™«å¼•æ“æ­£å¸¸å·¥ä½œ")
        print("âœ… æ–°é—»æŠ“å–åŠŸèƒ½æ­£å¸¸")
        print("âœ… AIå†…å®¹å¤„ç†åŠŸèƒ½æ­£å¸¸")
        print("âœ… æ•°æ®æ ¼å¼è½¬æ¢æ­£å¸¸")
        print("âš ï¸ é£ä¹¦å†™å…¥éœ€è¦æ­£ç¡®çš„é…ç½®")
        
        print("\nğŸ”§ ä¸‹ä¸€æ­¥éœ€è¦åšçš„ï¼š")
        print("1. è·å–æ­£ç¡®çš„é£ä¹¦å¤šç»´è¡¨æ ¼token")
        print("2. é…ç½®åº”ç”¨æƒé™")
        print("3. ç¡®ä¿è¡¨æ ¼å­—æ®µåç§°åŒ¹é…")
        print("4. æµ‹è¯•å®Œæ•´çš„å†™å…¥åŠŸèƒ½")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # æ¸…ç†èµ„æº
        await crawler.cleanup()
        print("\nğŸ§¹ èµ„æºæ¸…ç†å®Œæˆ")

if __name__ == "__main__":
    asyncio.run(demo_full_workflow())
