#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ÊîπËøõÁöÑÁà¨Ëô´ÊµãËØï - Êõ¥Â•ΩÁöÑÊñáÁ´†ÊèêÂèñ
"""

import asyncio
import sys
from pathlib import Path

# Ê∑ªÂä†È°πÁõÆÊ†πÁõÆÂΩïÂà∞PythonË∑ØÂæÑ
sys.path.append(str(Path(__file__).parent))

from crawler.main import AINewsCrawler
from crawler.content_processor import ContentProcessor

async def improved_crawler_test():
    """ÊîπËøõÁöÑÁà¨Ëô´ÊµãËØï"""
    
    print("üï∑Ô∏è ÊîπËøõÁöÑÁà¨Ëô´ÊµãËØï - Êõ¥Â•ΩÁöÑÊñáÁ´†ÊèêÂèñ")
    print("=" * 60)
    
    # ÂàõÂª∫Áà¨Ëô´ÂÆû‰æã
    crawler = AINewsCrawler()
    
    try:
        # ÂàùÂßãÂåñÁà¨Ëô´
        await crawler.init_crawler()
        print("‚úÖ Áà¨Ëô´ÂºïÊìéÂàùÂßãÂåñÊàêÂäü")
        
        # ÊµãËØïÊõ¥Â§öÁΩëÁ´ô
        test_sources = [
            {"name": "36Ê∞™", "url": "https://36kr.com", "type": "tech"},
            {"name": "ÈáèÂ≠ê‰Ωç", "url": "https://www.qbitai.com", "type": "ai"},
            {"name": "CSDN", "url": "https://www.csdn.net", "type": "ai"},
            {"name": "ÊéòÈáë", "url": "https://juejin.cn", "type": "tech"},
            {"name": "Â∞ëÊï∞Ê¥æ", "url": "https://sspai.com", "type": "tech"}
        ]
        
        print(f"\nüîç ÂºÄÂßãÁà¨Âèñ {len(test_sources)} ‰∏™ÁΩëÁ´ô...")
        
        all_articles = []
        
        for source in test_sources:
            print(f"\nüì∞ Ê≠£Âú®Áà¨Âèñ: {source['name']} ({source['url']})")
            
            try:
                # ÊâßË°åÁà¨ÂèñÔºå‰ΩøÁî®Êõ¥ÈïøÁöÑÁ≠âÂæÖÊó∂Èó¥ÂíåÈáçËØïÊú∫Âà∂
                max_retries = 2
                result = None
                
                for attempt in range(max_retries):
                    try:
                        print(f"  üîÑ Â∞ùËØï {attempt + 1}/{max_retries}...")
                        result = await crawler.crawler.arun(
                            url=source['url'],
                            wait_for="networkidle",
                            delay_before_return_html=3,
                            timeout=30000,  # 30ÁßíË∂ÖÊó∂
                            js_code="""
                            // Á≠âÂæÖÈ°µÈù¢Âä†ËΩΩÂÆåÊàê
                            await new Promise(resolve => setTimeout(resolve, 2000));
                            
                            // Â∞ùËØïÊªöÂä®È°µÈù¢‰ª•Ëß¶ÂèëÊáíÂä†ËΩΩ
                            window.scrollTo(0, document.body.scrollHeight);
                            await new Promise(resolve => setTimeout(resolve, 1500));
                            window.scrollTo(0, 0);
                            
                            // Â∞ùËØïÁÇπÂáª"Âä†ËΩΩÊõ¥Â§ö"ÊåâÈíÆ
                            const loadMoreBtn = document.querySelector('[class*="load"], [class*="more"], [class*="Â±ïÂºÄ"]');
                            if (loadMoreBtn) {
                                loadMoreBtn.click();
                                await new Promise(resolve => setTimeout(resolve, 1500));
                            }
                            """
                        )
                        if result.success:
                            break
                    except Exception as e:
                        print(f"  ‚ö†Ô∏è Â∞ùËØï {attempt + 1} Â§±Ë¥•: {e}")
                        if attempt < max_retries - 1:
                            print(f"  ‚è≥ Á≠âÂæÖ2ÁßíÂêéÈáçËØï...")
                            await asyncio.sleep(2)
                        else:
                            print(f"  ‚ùå ÊâÄÊúâÈáçËØïÈÉΩÂ§±Ë¥•‰∫Ü")
                
                if result.success:
                    print(f"‚úÖ {source['name']} Áà¨ÂèñÊàêÂäü")
                    print(f"üìÑ È°µÈù¢Ê†áÈ¢ò: {result.metadata.get('title', 'N/A')}")
                    print(f"üìä ÂÜÖÂÆπÈïøÂ∫¶: {len(result.html)} Â≠óÁ¨¶")
                    
                    # ÊèêÂèñÊñáÁ´†‰ø°ÊÅØ
                    articles = extract_articles_improved(result.html, source['name'], source['url'])
                    
                    print(f"üì∞ ÊèêÂèñÂà∞ {len(articles)} ÁØáÊñáÁ´†")
                    
                    # ÊòæÁ§∫Ââç3ÁØáÊñáÁ´†
                    for i, article in enumerate(articles[:3], 1):
                        print(f"  {i}. {article['title'][:60]}...")
                        print(f"     Êù•Ê∫ê: {article['source']}")
                        print(f"     ÊëòË¶Å: {article['summary'][:100]}...")
                        print()
                    
                    all_articles.extend(articles)
                    
                else:
                    print(f"‚ùå {source['name']} Áà¨ÂèñÂ§±Ë¥•")
                    
            except Exception as e:
                print(f"‚ùå {source['name']} Áà¨ÂèñÂá∫Èîô: {e}")
        
        print(f"\nüìä Áà¨ÂèñÁªüËÆ°:")
        print(f"ÊÄªÊñáÁ´†Êï∞: {len(all_articles)}")
        print(f"ÊàêÂäüÁΩëÁ´ô: {len([s for s in test_sources if any(a['source'] == s['name'] for a in all_articles)])}")
        
        if all_articles:
            print(f"\nü§ñ ‰ΩøÁî®ÁÅ´Â±±ÊñπËàüDeepSeekÂ§ÑÁêÜ {len(all_articles)} ÁØáÊñáÁ´†...")
            
            # ‰ΩøÁî®ÂÜÖÂÆπÂ§ÑÁêÜÂô®Â§ÑÁêÜÊñáÁ´†
            processor = ContentProcessor()
            result = await processor.process_articles(all_articles[:15])  # Â§ÑÁêÜÂâç15ÁØá
            
            print("‚úÖ ÂÜÖÂÆπÂ§ÑÁêÜÂÆåÊàêÔºÅ")
            print("\nüìã Êó©Êä•ÊëòË¶Å:")
            print("-" * 50)
            print(result['summary'])
            
            print("\nüìà ÂèëÂ±ïË∂ãÂäø:")
            print("-" * 50)
            for i, trend in enumerate(result['trends'][:5], 1):
                print(f"{i}. {trend}")
            
            print("\nüé® ÂõæÁâáÊèêÁ§∫ËØç:")
            print("-" * 50)
            for i, prompt in enumerate(result['image_prompts'], 1):
                print(f"{i}. {prompt}")
            
            print(f"\nüìä Â§ÑÁêÜÁªüËÆ°:")
            print(f"Â§ÑÁêÜÊó∂Èó¥: {result['processed_at']}")
            print(f"ÊñáÁ´†ÊÄªÊï∞: {result['total_articles']}")
            print(f"‰øùÁïôÊñáÁ´†: {len(result['articles'])}")
        
    except Exception as e:
        print(f"‚ùå ÊµãËØïÂ§±Ë¥•: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # Ê∏ÖÁêÜËµÑÊ∫ê
        await crawler.cleanup()
        print("\nüßπ ËµÑÊ∫êÊ∏ÖÁêÜÂÆåÊàê")

def extract_articles_improved(html, source_name, base_url):
    """ÊîπËøõÁöÑÊñáÁ´†ÊèêÂèñÂáΩÊï∞"""
    articles = []
    
    try:
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(html, 'html.parser')
        
        if source_name == "36Ê∞™":
            # 36Ê∞™ÁöÑÊñáÁ´†ÈÄâÊã©Âô® - Êõ¥Á≤æÁ°Æ
            selectors = [
                'div[class*="article-item"]',
                'div[class*="news-item"]',
                'div[class*="item"]',
                'article',
                'div[class*="card"]'
            ]
            
            for selector in selectors:
                elements = soup.select(selector)
                if elements:
                    print(f"  üîç ‰ΩøÁî®ÈÄâÊã©Âô®: {selector} (ÊâæÂà∞ {len(elements)} ‰∏™ÂÖÉÁ¥†)")
                    break
            
            for element in elements[:10]:  # Âè™ÂèñÂâç10ÁØá
                title_elem = element.find(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']) or element.find('a')
                if title_elem:
                    title = title_elem.get_text().strip()
                    if len(title) < 10:  # ËøáÊª§Â§™Áü≠ÁöÑÊ†áÈ¢ò
                        continue
                    
                    # Ëé∑ÂèñÈìæÊé•
                    link_elem = title_elem.find('a') if title_elem.name != 'a' else title_elem
                    url = link_elem.get('href', '') if link_elem else ''
                    if url and not url.startswith('http'):
                        url = 'https://36kr.com' + url
                    
                    # Ëé∑ÂèñÊëòË¶Å
                    summary_elem = element.find(['p', 'div'], class_=lambda x: x and 'summary' in x.lower()) or element.find('p')
                    summary = summary_elem.get_text().strip() if summary_elem else title
                    
                    articles.append({
                        'title': title,
                        'summary': summary,
                        'source': source_name,
                        'url': url,
                        'publish_time': '2024-01-15T10:00:00Z'
                    })
        
        elif source_name == "ËôéÂóÖÁΩë":
            # ËôéÂóÖÁΩëÁöÑÊñáÁ´†ÈÄâÊã©Âô®
            selectors = [
                'div[class*="article-item"]',
                'div[class*="news-item"]',
                'article',
                'div[class*="item"]'
            ]
            
            for selector in selectors:
                elements = soup.select(selector)
                if elements:
                    print(f"  üîç ‰ΩøÁî®ÈÄâÊã©Âô®: {selector} (ÊâæÂà∞ {len(elements)} ‰∏™ÂÖÉÁ¥†)")
                    break
            
            for element in elements[:10]:
                title_elem = element.find(['h1', 'h2', 'h3', 'h4'])
                if title_elem:
                    title = title_elem.get_text().strip()
                    if len(title) < 10:
                        continue
                    
                    link_elem = title_elem.find('a') or element.find('a')
                    url = link_elem.get('href', '') if link_elem else ''
                    if url and not url.startswith('http'):
                        url = 'https://www.huxiu.com' + url
                    
                    summary_elem = element.find('p') or element.find('div', class_=lambda x: x and 'summary' in x.lower())
                    summary = summary_elem.get_text().strip() if summary_elem else title
                    
                    articles.append({
                        'title': title,
                        'summary': summary,
                        'source': source_name,
                        'url': url,
                        'publish_time': '2024-01-15T10:00:00Z'
                    })
        
        elif source_name == "ÈáèÂ≠ê‰Ωç":
            # ÈáèÂ≠ê‰ΩçÁöÑÊñáÁ´†ÈÄâÊã©Âô®
            selectors = [
                'div[class*="article-item"]',
                'div[class*="news-item"]',
                'article',
                'div[class*="item"]'
            ]
            
            for selector in selectors:
                elements = soup.select(selector)
                if elements:
                    print(f"  üîç ‰ΩøÁî®ÈÄâÊã©Âô®: {selector} (ÊâæÂà∞ {len(elements)} ‰∏™ÂÖÉÁ¥†)")
                    break
            
            for element in elements[:10]:
                title_elem = element.find(['h1', 'h2', 'h3', 'h4'])
                if title_elem:
                    title = title_elem.get_text().strip()
                    if len(title) < 10:
                        continue
                    
                    link_elem = title_elem.find('a') or element.find('a')
                    url = link_elem.get('href', '') if link_elem else ''
                    if url and not url.startswith('http'):
                        url = 'https://www.qbitai.com' + url
                    
                    summary_elem = element.find('p') or element.find('div', class_=lambda x: x and 'summary' in x.lower())
                    summary = summary_elem.get_text().strip() if summary_elem else title
                    
                    articles.append({
                        'title': title,
                        'summary': summary,
                        'source': source_name,
                        'url': url,
                        'publish_time': '2024-01-15T10:00:00Z'
                    })
        
        elif source_name == "CSDN":
            # CSDNÁöÑÊñáÁ´†ÈÄâÊã©Âô® - Êõ¥Á≤æÁ°ÆÁöÑÈÄâÊã©Âô®
            selectors = [
                'div[class*="blog-list-box"] div[class*="blog-list-item"]',
                'div[class*="article-item"]',
                'div[class*="news-item"]',
                'article',
                'div[class*="item"]',
                'div[class*="blog"]'
            ]
            
            for selector in selectors:
                elements = soup.select(selector)
                if elements:
                    print(f"  üîç ‰ΩøÁî®ÈÄâÊã©Âô®: {selector} (ÊâæÂà∞ {len(elements)} ‰∏™ÂÖÉÁ¥†)")
                    break
            
            for element in elements[:10]:
                # Â∞ùËØïÂ§öÁßçÊ†áÈ¢òÈÄâÊã©Âô®
                title_elem = (element.find(['h1', 'h2', 'h3', 'h4']) or 
                             element.find('a', class_=lambda x: x and 'title' in x.lower()) or
                             element.find('a', class_=lambda x: x and 'blog' in x.lower()))
                
                if title_elem:
                    title = title_elem.get_text().strip()
                    if len(title) < 10:
                        continue
                    
                    # Ëé∑ÂèñÈìæÊé•
                    link_elem = title_elem if title_elem.name == 'a' else title_elem.find('a') or element.find('a')
                    url = link_elem.get('href', '') if link_elem else ''
                    if url and not url.startswith('http'):
                        url = 'https://www.csdn.net' + url
                    
                    # Ëé∑ÂèñÊëòË¶Å
                    summary_elem = (element.find('p') or 
                                   element.find('div', class_=lambda x: x and 'summary' in x.lower()) or
                                   element.find('div', class_=lambda x: x and 'desc' in x.lower()))
                    summary = summary_elem.get_text().strip() if summary_elem else title
                    
                    articles.append({
                        'title': title,
                        'summary': summary,
                        'source': source_name,
                        'url': url,
                        'publish_time': '2024-01-15T10:00:00Z'
                    })
        
        elif source_name == "InfoQ":
            # InfoQÁöÑÊñáÁ´†ÈÄâÊã©Âô®
            selectors = [
                'div[class*="article-item"]',
                'div[class*="news-item"]',
                'article',
                'div[class*="item"]'
            ]
            
            for selector in selectors:
                elements = soup.select(selector)
                if elements:
                    print(f"  üîç ‰ΩøÁî®ÈÄâÊã©Âô®: {selector} (ÊâæÂà∞ {len(elements)} ‰∏™ÂÖÉÁ¥†)")
                    break
            
            for element in elements[:10]:
                title_elem = element.find(['h1', 'h2', 'h3', 'h4'])
                if title_elem:
                    title = title_elem.get_text().strip()
                    if len(title) < 10:
                        continue
                    
                    link_elem = title_elem.find('a') or element.find('a')
                    url = link_elem.get('href', '') if link_elem else ''
                    if url and not url.startswith('http'):
                        url = 'https://www.infoq.cn' + url
                    
                    summary_elem = element.find('p') or element.find('div', class_=lambda x: x and 'summary' in x.lower())
                    summary = summary_elem.get_text().strip() if summary_elem else title
                    
                    articles.append({
                        'title': title,
                        'summary': summary,
                        'source': source_name,
                        'url': url,
                        'publish_time': '2024-01-15T10:00:00Z'
                    })
        
        elif source_name == "ÊéòÈáë":
            # ÊéòÈáëÁöÑÊñáÁ´†ÈÄâÊã©Âô®
            selectors = [
                'div[class*="entry-list"] div[class*="item"]',
                'div[class*="article-item"]',
                'article',
                'div[class*="item"]'
            ]
            
            for selector in selectors:
                elements = soup.select(selector)
                if elements:
                    print(f"  üîç ‰ΩøÁî®ÈÄâÊã©Âô®: {selector} (ÊâæÂà∞ {len(elements)} ‰∏™ÂÖÉÁ¥†)")
                    break
            
            for element in elements[:10]:
                title_elem = element.find(['h1', 'h2', 'h3', 'h4']) or element.find('a', class_=lambda x: x and 'title' in x.lower())
                if title_elem:
                    title = title_elem.get_text().strip()
                    if len(title) < 10:
                        continue
                    
                    link_elem = title_elem if title_elem.name == 'a' else title_elem.find('a') or element.find('a')
                    url = link_elem.get('href', '') if link_elem else ''
                    if url and not url.startswith('http'):
                        url = 'https://juejin.cn' + url
                    
                    summary_elem = element.find('p') or element.find('div', class_=lambda x: x and 'summary' in x.lower())
                    summary = summary_elem.get_text().strip() if summary_elem else title
                    
                    articles.append({
                        'title': title,
                        'summary': summary,
                        'source': source_name,
                        'url': url,
                        'publish_time': '2024-01-15T10:00:00Z'
                    })
        
        elif source_name == "Â∞ëÊï∞Ê¥æ":
            # Â∞ëÊï∞Ê¥æÁöÑÊñáÁ´†ÈÄâÊã©Âô®
            selectors = [
                'div[class*="article-item"]',
                'article',
                'div[class*="item"]',
                'div[class*="post"]'
            ]
            
            for selector in selectors:
                elements = soup.select(selector)
                if elements:
                    print(f"  üîç ‰ΩøÁî®ÈÄâÊã©Âô®: {selector} (ÊâæÂà∞ {len(elements)} ‰∏™ÂÖÉÁ¥†)")
                    break
            
            for element in elements[:10]:
                title_elem = element.find(['h1', 'h2', 'h3', 'h4']) or element.find('a', class_=lambda x: x and 'title' in x.lower())
                if title_elem:
                    title = title_elem.get_text().strip()
                    if len(title) < 10:
                        continue
                    
                    link_elem = title_elem if title_elem.name == 'a' else title_elem.find('a') or element.find('a')
                    url = link_elem.get('href', '') if link_elem else ''
                    if url and not url.startswith('http'):
                        url = 'https://sspai.com' + url
                    
                    summary_elem = element.find('p') or element.find('div', class_=lambda x: x and 'summary' in x.lower())
                    summary = summary_elem.get_text().strip() if summary_elem else title
                    
                    articles.append({
                        'title': title,
                        'summary': summary,
                        'source': source_name,
                        'url': url,
                        'publish_time': '2024-01-15T10:00:00Z'
                    })
        
        # Â¶ÇÊûúÊ≤°ÊúâÊâæÂà∞ÁâπÂÆöÈÄâÊã©Âô®Ôºå‰ΩøÁî®ÈÄöÁî®ÈÄâÊã©Âô®
        if not articles:
            print(f"  üîç ‰ΩøÁî®ÈÄöÁî®ÈÄâÊã©Âô®...")
            # ÈÄöÁî®ÊñáÁ´†ÊèêÂèñ
            for element in soup.find_all(['article', 'div'], class_=lambda x: x and any(keyword in x.lower() for keyword in ['article', 'news', 'item', 'card']))[:15]:
                title_elem = element.find(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
                if title_elem:
                    title = title_elem.get_text().strip()
                    if len(title) < 10:
                        continue
                    
                    link_elem = title_elem.find('a') or element.find('a')
                    url = link_elem.get('href', '') if link_elem else ''
                    
                    summary_elem = element.find('p')
                    summary = summary_elem.get_text().strip() if summary_elem else title
                    
                    articles.append({
                        'title': title,
                        'summary': summary,
                        'source': source_name,
                        'url': url,
                        'publish_time': '2024-01-15T10:00:00Z'
                    })
    
    except Exception as e:
        print(f"‚ö†Ô∏è ÊèêÂèñÊñáÁ´†Êó∂Âá∫Èîô: {e}")
    
    return articles

if __name__ == "__main__":
    asyncio.run(improved_crawler_test())

