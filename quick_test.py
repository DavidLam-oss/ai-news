#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¿«é€Ÿæµ‹è¯•è„šæœ¬ - éªŒè¯ç³»ç»ŸåŠŸèƒ½
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent))

async def quick_test():
    """å¿«é€Ÿæµ‹è¯•ç³»ç»ŸåŠŸèƒ½"""
    print("ğŸš€ AIæ—©æŠ¥ç³»ç»Ÿå¿«é€Ÿæµ‹è¯•")
    print("=" * 40)
    
    try:
        # æµ‹è¯•çˆ¬è™«åŠŸèƒ½
        print("\n1ï¸âƒ£ æµ‹è¯•çˆ¬è™«åŠŸèƒ½...")
        from crawler.main import AINewsCrawler
        
        crawler = AINewsCrawler()
        await crawler.init_crawler()
        
        # åªçˆ¬å–ä¸€ä¸ªç½‘ç«™è¿›è¡Œå¿«é€Ÿæµ‹è¯•
        from crawler.news_sources import NewsSources
        news_sources = NewsSources()
        sources = news_sources.get_sources()[:1]  # åªæµ‹è¯•ç¬¬ä¸€ä¸ªæº
        
        articles = []
        for source in sources:
            print(f"   æ­£åœ¨çˆ¬å–: {source['name']}")
            result = await crawler.crawler.arun(
                url=source['url'],
                wait_for="networkidle",
                delay_before_return_html=2
            )
            
            if result.success:
                from improved_crawler_test import extract_articles_improved
                extracted = extract_articles_improved(result.html, source['name'], source['url'])
                articles.extend(extracted)
                print(f"   âœ… è·å–åˆ° {len(extracted)} ç¯‡æ–‡ç« ")
            else:
                print(f"   âŒ çˆ¬å–å¤±è´¥")
        
        await crawler.cleanup()
        
        if articles:
            print(f"âœ… çˆ¬è™«æµ‹è¯•é€šè¿‡ï¼Œå…±è·å– {len(articles)} ç¯‡æ–‡ç« ")
        else:
            print("âŒ çˆ¬è™«æµ‹è¯•å¤±è´¥ï¼Œæœªè·å–åˆ°æ–‡ç« ")
            return
        
        # æµ‹è¯•AIå¤„ç†åŠŸèƒ½
        print("\n2ï¸âƒ£ æµ‹è¯•AIå¤„ç†åŠŸèƒ½...")
        from crawler.content_processor import ContentProcessor
        
        processor = ContentProcessor()
        result = await processor.process_articles(articles[:3])  # åªå¤„ç†å‰3ç¯‡
        
        if result and result.get('summary'):
            print("âœ… AIå¤„ç†æµ‹è¯•é€šè¿‡")
            print(f"   æ‘˜è¦é•¿åº¦: {len(result['summary'])} å­—ç¬¦")
            print(f"   è¶‹åŠ¿æ•°é‡: {len(result['trends'])}")
            print(f"   å›¾ç‰‡æç¤ºè¯æ•°é‡: {len(result['image_prompts'])}")
        else:
            print("âŒ AIå¤„ç†æµ‹è¯•å¤±è´¥")
            return
        
        # æµ‹è¯•é£ä¹¦è¿æ¥
        print("\n3ï¸âƒ£ æµ‹è¯•é£ä¹¦è¿æ¥...")
        import os
        os.environ["FEISHU_APP_ID"] = "cli_a8366b7ef13a100c"
        os.environ["FEISHU_APP_SECRET"] = "5nkWuj9xfU5bjg0qEJBcKhmX1H1ptjvr"
        os.environ["FEISHU_BASE_URL"] = "https://open.feishu.cn/open-apis"
        
        from feishu.client import FeishuClient
        client = FeishuClient()
        
        try:
            access_token = await client.get_access_token()
            if access_token:
                print("âœ… é£ä¹¦è¿æ¥æµ‹è¯•é€šè¿‡")
                print(f"   è®¿é—®ä»¤ç‰Œ: {access_token[:20]}...")
            else:
                print("âŒ é£ä¹¦è¿æ¥æµ‹è¯•å¤±è´¥")
        except Exception as e:
            print(f"âŒ é£ä¹¦è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
        finally:
            await client.close()
        
        print("\nğŸ‰ å¿«é€Ÿæµ‹è¯•å®Œæˆï¼")
        print("\nğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“ï¼š")
        print("âœ… çˆ¬è™«åŠŸèƒ½æ­£å¸¸")
        print("âœ… AIå¤„ç†åŠŸèƒ½æ­£å¸¸")
        print("âœ… é£ä¹¦è¿æ¥æ­£å¸¸")
        print("âš ï¸ é£ä¹¦å†™å…¥éœ€è¦æ­£ç¡®çš„è¡¨æ ¼é…ç½®")
        
        print("\nğŸ’¡ ç³»ç»Ÿå·²å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥ï¼š")
        print("1. è¿è¡Œå®Œæ•´çˆ¬è™«: python3 crawler/main.py")
        print("2. è¿è¡Œæ¼”ç¤ºè„šæœ¬: python3 demo_full_workflow.py")
        print("3. é…ç½®æ­£ç¡®çš„é£ä¹¦è¡¨æ ¼tokenåæµ‹è¯•å†™å…¥åŠŸèƒ½")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(quick_test())

