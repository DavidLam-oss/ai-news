#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–çš„çˆ¬è™«æµ‹è¯• - ç›´æ¥ä½¿ç”¨æˆ‘ä»¬ä¹‹å‰æˆåŠŸçš„é€»è¾‘
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent))

from crawler.main import AINewsCrawler
from crawler.content_processor import ContentProcessor

async def simple_crawler_test():
    """ç®€åŒ–çš„çˆ¬è™«æµ‹è¯•"""
    
    print("ğŸ•·ï¸ ç®€åŒ–çˆ¬è™«æµ‹è¯• - ä½¿ç”¨æˆåŠŸçš„é€»è¾‘")
    print("=" * 60)
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = AINewsCrawler()
    
    try:
        # åˆå§‹åŒ–çˆ¬è™«
        await crawler.init_crawler()
        print("âœ… çˆ¬è™«å¼•æ“åˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•å•ä¸ªç½‘ç«™
        test_url = "https://36kr.com"
        print(f"\nğŸ“° æ­£åœ¨çˆ¬å–: {test_url}")
        
        # ç›´æ¥ä½¿ç”¨crawler.arunæ–¹æ³•
        result = await crawler.crawler.arun(
            url=test_url,
            wait_for="networkidle",
            delay_before_return_html=3,
            js_code="""
            // ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
            await new Promise(resolve => setTimeout(resolve, 2000));
            
            // å°è¯•æ»šåŠ¨é¡µé¢ä»¥è§¦å‘æ‡’åŠ è½½
            window.scrollTo(0, document.body.scrollHeight);
            await new Promise(resolve => setTimeout(resolve, 1500));
            window.scrollTo(0, 0);
            """
        )
        
        if result.success:
            print(f"âœ… çˆ¬å–æˆåŠŸ")
            print(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {result.metadata.get('title', 'N/A')}")
            print(f"ğŸ“Š å†…å®¹é•¿åº¦: {len(result.html)} å­—ç¬¦")
            
            # ä½¿ç”¨æˆ‘ä»¬ä¹‹å‰æˆåŠŸçš„æ–‡ç« æå–é€»è¾‘
            from improved_crawler_test import extract_articles_improved
            articles = extract_articles_improved(result.html, "36æ°ª", test_url)
            
            print(f"ğŸ“° æå–åˆ° {len(articles)} ç¯‡æ–‡ç« ")
            
            # æ˜¾ç¤ºæ–‡ç« 
            for i, article in enumerate(articles[:3], 1):
                print(f"  {i}. {article['title'][:60]}...")
                print(f"     æ¥æº: {article['source']}")
                print(f"     æ‘˜è¦: {article['summary'][:100]}...")
                print()
            
            if articles:
                print(f"\nğŸ¤– ä½¿ç”¨DeepSeekå¤„ç† {len(articles)} ç¯‡æ–‡ç« ...")
                
                # ä½¿ç”¨å†…å®¹å¤„ç†å™¨å¤„ç†æ–‡ç« 
                processor = ContentProcessor()
                result = await processor.process_articles(articles[:5])  # å¤„ç†å‰5ç¯‡
                
                print("âœ… å†…å®¹å¤„ç†å®Œæˆï¼")
                print("\nğŸ“‹ æ—©æŠ¥æ‘˜è¦:")
                print("-" * 50)
                print(result['summary'])
                
                print("\nğŸ“ˆ å‘å±•è¶‹åŠ¿:")
                print("-" * 50)
                for i, trend in enumerate(result['trends'][:3], 1):
                    print(f"{i}. {trend}")
                
                print("\nğŸ¨ å›¾ç‰‡æç¤ºè¯:")
                print("-" * 50)
                for i, prompt in enumerate(result['image_prompts'], 1):
                    print(f"{i}. {prompt}")
        
        else:
            print(f"âŒ çˆ¬å–å¤±è´¥")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # æ¸…ç†èµ„æº
        await crawler.cleanup()
        print("\nğŸ§¹ èµ„æºæ¸…ç†å®Œæˆ")

if __name__ == "__main__":
    asyncio.run(simple_crawler_test())
