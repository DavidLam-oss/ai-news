#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´å·¥ä½œæµç¨‹æµ‹è¯• - çˆ¬è™« + AIå¤„ç† + é£ä¹¦å†™å…¥
"""

import asyncio
import json
import os
import sys
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent))

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["FEISHU_APP_ID"] = "cli_a8366b7ef13a100c"
os.environ["FEISHU_APP_SECRET"] = "5nkWuj9xfU5bjg0qEJBcKhmX1H1ptjvr"
os.environ["FEISHU_BASE_URL"] = "https://open.feishu.cn/open-apis"
os.environ["FEISHU_TABLE_TOKEN"] = "F5I2bdNZxawzTqsRBVbcJWEMn9H"

from crawler.main import AINewsCrawler
from improved_crawler_test import extract_articles_improved

async def test_complete_workflow():
    """æµ‹è¯•å®Œæ•´å·¥ä½œæµç¨‹"""
    print("ğŸš€ å®Œæ•´å·¥ä½œæµç¨‹æµ‹è¯• - çˆ¬è™« + AIå¤„ç† + é£ä¹¦å†™å…¥")
    print("=" * 70)
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = AINewsCrawler()
    
    try:
        # ç¬¬ä¸€æ­¥ï¼šåˆå§‹åŒ–çˆ¬è™«
        print("\nğŸ“¡ ç¬¬ä¸€æ­¥ï¼šåˆå§‹åŒ–çˆ¬è™«å¼•æ“...")
        await crawler.init_crawler()
        print("âœ… çˆ¬è™«å¼•æ“åˆå§‹åŒ–æˆåŠŸ")
        
        # ç¬¬äºŒæ­¥ï¼šçˆ¬å–æ–°é—»ï¼ˆä½¿ç”¨ç®€åŒ–çš„æ–¹æ³•ï¼‰
        print("\nğŸ•·ï¸ ç¬¬äºŒæ­¥ï¼šçˆ¬å–AIç§‘æŠ€æ–°é—»...")
        
        # ä½¿ç”¨å·²çŸ¥æœ‰æ•ˆçš„ç½‘ç«™è¿›è¡Œæµ‹è¯•
        test_url = "https://36kr.com"
        print(f"   æ­£åœ¨çˆ¬å–: {test_url}")
        
        result = await crawler.crawler.arun(
            url=test_url,
            wait_for="networkidle",
            delay_before_return_html=2
        )
        
        if result.success:
            print("âœ… çˆ¬å–æˆåŠŸ")
            print(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {result.metadata.get('title', 'N/A')}")
            print(f"ğŸ“Š å†…å®¹é•¿åº¦: {len(result.html)} å­—ç¬¦")
            
            # æå–æ–‡ç« 
            articles = extract_articles_improved(result.html, "36æ°ª", test_url)
            print(f"ğŸ“° æå–åˆ° {len(articles)} ç¯‡æ–‡ç« ")
            
            if articles:
                # æ˜¾ç¤ºå‰3ç¯‡æ–‡ç« 
                for i, article in enumerate(articles[:3], 1):
                    print(f"  {i}. {article['title'][:60]}...")
                    print(f"     æ¥æº: {article['source']}")
                    print(f"     æ‘˜è¦: {article['summary'][:80]}...")
                    print()
            else:
                print("âŒ æœªæå–åˆ°æ–‡ç« ")
                return
        else:
            print("âŒ çˆ¬å–å¤±è´¥")
            return
        
        # ç¬¬ä¸‰æ­¥ï¼šAIå¤„ç†å†…å®¹
        print("\nğŸ¤– ç¬¬ä¸‰æ­¥ï¼šAIå¤„ç†å†…å®¹...")
        report = await crawler.generate_daily_report(articles)
        
        print("âœ… AIå†…å®¹å¤„ç†å®Œæˆ")
        print(f"ğŸ“… æ—©æŠ¥æ—¥æœŸ: {report['date']}")
        print(f"ğŸ“ æ—©æŠ¥æ ‡é¢˜: {report['title']}")
        
        # æ˜¾ç¤ºå¤„ç†ç»“æœ
        print("\nğŸ“‹ æ—©æŠ¥æ‘˜è¦:")
        print("-" * 50)
        print(report['summary'])
        
        print("\nğŸ“ˆ å‘å±•è¶‹åŠ¿:")
        print("-" * 50)
        for i, trend in enumerate(report['trends'][:3], 1):
            print(f"{i}. {trend}")
        
        print("\nğŸ¨ å›¾ç‰‡æç¤ºè¯:")
        print("-" * 50)
        for i, prompt in enumerate(report['image_prompts'], 1):
            print(f"{i}. {prompt}")
        
        # ç¬¬å››æ­¥ï¼šå†™å…¥é£ä¹¦
        print("\nğŸ’¾ ç¬¬å››æ­¥ï¼šå†™å…¥é£ä¹¦å¤šç»´è¡¨æ ¼...")
        
        try:
            success = await crawler.save_to_feishu(report)
            
            if success:
                print("âœ… æ•°æ®æˆåŠŸå†™å…¥é£ä¹¦å¤šç»´è¡¨æ ¼ï¼")
                print("ğŸ‰ å®Œæ•´å·¥ä½œæµç¨‹æµ‹è¯•æˆåŠŸï¼")
                
                print("\nğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“:")
                print("âœ… çˆ¬è™«å¼•æ“æ­£å¸¸å·¥ä½œ")
                print("âœ… æ–°é—»æŠ“å–åŠŸèƒ½æ­£å¸¸")
                print("âœ… AIå†…å®¹å¤„ç†åŠŸèƒ½æ­£å¸¸")
                print("âœ… é£ä¹¦APIè¿æ¥æ­£å¸¸")
                print("âœ… æ•°æ®å†™å…¥åŠŸèƒ½æ­£å¸¸")
                print("âœ… å®Œæ•´å·¥ä½œæµç¨‹æ­£å¸¸")
                
            else:
                print("âŒ é£ä¹¦å†™å…¥å¤±è´¥")
                print("ğŸ’¡ è¯·æ£€æŸ¥åº”ç”¨æƒé™é…ç½®")
                
        except Exception as e:
            print(f"âŒ é£ä¹¦å†™å…¥è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # æ¸…ç†èµ„æº
        await crawler.cleanup()
        print("\nğŸ§¹ èµ„æºæ¸…ç†å®Œæˆ")

if __name__ == "__main__":
    asyncio.run(test_complete_workflow())
