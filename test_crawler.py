#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•çˆ¬è™«åŠŸèƒ½
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent))

from crawler.main import AINewsCrawler
from crawler.content_processor import ContentProcessor

async def test_crawler():
    """æµ‹è¯•çˆ¬è™«åŠŸèƒ½"""
    
    print("ğŸ•·ï¸ å¼€å§‹æµ‹è¯•çˆ¬è™«åŠŸèƒ½...")
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = AINewsCrawler()
    
    try:
        # åˆå§‹åŒ–çˆ¬è™«
        await crawler.init_crawler()
        print("âœ… çˆ¬è™«å¼•æ“åˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•çˆ¬å–ä¸€ä¸ªç®€å•çš„ç½‘ç«™
        print("ğŸ” æ­£åœ¨çˆ¬å–æµ‹è¯•ç½‘ç«™...")
        
        # ä½¿ç”¨crawl4aiçˆ¬å–ä¸€ä¸ªç®€å•çš„AIæ–°é—»ç½‘ç«™
        from crawl4ai import AsyncWebCrawler
        
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•
        test_url = "https://www.36kr.com"
        
        print(f"ğŸ“° æ­£åœ¨çˆ¬å–: {test_url}")
        
        # æ‰§è¡Œçˆ¬å–
        result = await crawler.crawler.arun(
            url=test_url,
            wait_for="networkidle",
            delay_before_return_html=2
        )
        
        if result.success:
            print("âœ… çˆ¬å–æˆåŠŸï¼")
            print(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {result.metadata.get('title', 'N/A')}")
            print(f"ğŸ“Š å†…å®¹é•¿åº¦: {len(result.html)} å­—ç¬¦")
            
            # æå–ä¸€äº›æ–‡æœ¬å†…å®¹
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(result.html, 'html.parser')
            
            # æŸ¥æ‰¾å¯èƒ½çš„æ–°é—»æ ‡é¢˜
            titles = []
            for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
                if tag.get_text().strip():
                    titles.append(tag.get_text().strip())
            
            print(f"ğŸ“° æ‰¾åˆ° {len(titles)} ä¸ªæ ‡é¢˜:")
            for i, title in enumerate(titles[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"  {i}. {title}")
            
            # æ¨¡æ‹Ÿä¸€äº›æ–‡ç« æ•°æ®
            mock_articles = [
                {
                    "title": "AIæŠ€æœ¯çªç ´ï¼šå¤§è¯­è¨€æ¨¡å‹æ€§èƒ½å†åˆ›æ–°é«˜",
                    "summary": "æœ€æ–°ç ”ç©¶æ˜¾ç¤ºï¼Œå¤§è¯­è¨€æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸ºAIåº”ç”¨å¸¦æ¥æ–°çš„å¯èƒ½æ€§ã€‚",
                    "source": "36æ°ª",
                    "url": "https://example.com/ai-breakthrough",
                    "publish_time": "2024-01-15T10:00:00Z"
                },
                {
                    "title": "OpenAIå‘å¸ƒGPT-4 Turboï¼Œæˆæœ¬é™ä½50%",
                    "summary": "OpenAIå®£å¸ƒæ¨å‡ºGPT-4 Turboæ¨¡å‹ï¼Œåœ¨ä¿æŒé«˜è´¨é‡è¾“å‡ºçš„åŒæ—¶ï¼Œå¤§å¹…é™ä½äº†ä½¿ç”¨æˆæœ¬ã€‚",
                    "source": "æœºå™¨ä¹‹å¿ƒ",
                    "url": "https://example.com/gpt4-turbo",
                    "publish_time": "2024-01-15T11:00:00Z"
                },
                {
                    "title": "Google Geminiå¤šæ¨¡æ€AIèƒ½åŠ›çªå‡º",
                    "summary": "Googleå‘å¸ƒçš„Geminiæ¨¡å‹åœ¨å›¾åƒã€æ–‡æœ¬ã€éŸ³é¢‘ç­‰å¤šæ¨¡æ€ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æ¨¡å‹ã€‚",
                    "source": "é‡å­ä½",
                    "url": "https://example.com/gemini",
                    "publish_time": "2024-01-15T12:00:00Z"
                }
            ]
            
            print("\nğŸ¤– ä½¿ç”¨ç«å±±æ–¹èˆŸDeepSeekå¤„ç†å†…å®¹...")
            
            # ä½¿ç”¨å†…å®¹å¤„ç†å™¨å¤„ç†æ–‡ç« 
            processor = ContentProcessor()
            result = await processor.process_articles(mock_articles)
            
            print("âœ… å†…å®¹å¤„ç†å®Œæˆï¼")
            print("\nğŸ“‹ æ—©æŠ¥æ‘˜è¦:")
            print("-" * 50)
            print(result['summary'])
            
            print("\nğŸ“ˆ å‘å±•è¶‹åŠ¿:")
            print("-" * 50)
            for i, trend in enumerate(result['trends'], 1):
                print(f"{i}. {trend}")
            
            print("\nğŸ¨ å›¾ç‰‡æç¤ºè¯:")
            print("-" * 50)
            for i, prompt in enumerate(result['image_prompts'], 1):
                print(f"{i}. {prompt}")
                
        else:
            print("âŒ çˆ¬å–å¤±è´¥")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # æ¸…ç†èµ„æº
        await crawler.cleanup()
        print("ğŸ§¹ èµ„æºæ¸…ç†å®Œæˆ")

if __name__ == "__main__":
    asyncio.run(test_crawler())

