#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´å·¥ä½œæµç¨‹æµ‹è¯•è„šæœ¬
æµ‹è¯•ï¼šçˆ¬å– â†’ AIå¤„ç† â†’ å­˜å‚¨ â†’ æ¨é€
"""

import asyncio
import sys
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent))

from crawler.main import AINewsCrawler
from crawler.content_processor import ContentProcessor

async def test_full_workflow():
    """æµ‹è¯•å®Œæ•´å·¥ä½œæµç¨‹"""
    
    print("ğŸš€ AIæ—©æŠ¥ç³»ç»Ÿ - å®Œæ•´å·¥ä½œæµç¨‹æµ‹è¯•")
    print("=" * 60)
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = AINewsCrawler()
    
    try:
        # 1. åˆå§‹åŒ–çˆ¬è™«
        await crawler.init_crawler()
        print("âœ… 1. çˆ¬è™«å¼•æ“åˆå§‹åŒ–æˆåŠŸ")
        
        # 2. çˆ¬å–æ–°é—»
        print("\nğŸ“° 2. å¼€å§‹çˆ¬å–æ–°é—»...")
        articles = await crawler.crawl_news_sources()
        print(f"âœ… çˆ¬å–å®Œæˆï¼Œè·å–åˆ° {len(articles)} ç¯‡æ–‡ç« ")
        
        if not articles:
            print("âŒ æ²¡æœ‰è·å–åˆ°æ–‡ç« ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
            return
        
        # æ˜¾ç¤ºçˆ¬å–ç»“æœ
        for i, article in enumerate(articles[:3], 1):
            print(f"  {i}. {article['title'][:50]}...")
            print(f"     æ¥æº: {article['source']}")
            print()
        
        # 3. AIå†…å®¹å¤„ç†
        print("ğŸ¤– 3. å¼€å§‹AIå†…å®¹å¤„ç†...")
        processor = ContentProcessor()
        processed_result = await processor.process_articles(articles[:5])  # å¤„ç†å‰5ç¯‡
        
        print("âœ… AIå¤„ç†å®Œæˆï¼")
        print(f"ğŸ“‹ æ—©æŠ¥æ‘˜è¦: {processed_result['summary'][:100]}...")
        print(f"ğŸ“ˆ è¶‹åŠ¿åˆ†æ: {len(processed_result['trends'])} ä¸ªè¶‹åŠ¿")
        print(f"ğŸ¨ å›¾ç‰‡æç¤ºè¯: {len(processed_result['image_prompts'])} ä¸ª")
        
        # 4. æµ‹è¯•é£ä¹¦å­˜å‚¨ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
        print("\nğŸ“Š 4. æµ‹è¯•é£ä¹¦å­˜å‚¨...")
        try:
            # å‡†å¤‡è®°å½•æ•°æ®
            record_data = {
                'æ—¥æœŸ': datetime.now().strftime('%Y-%m-%d'),
                'æ—©æŠ¥åŸå§‹å†…å®¹': f"çˆ¬å–åˆ°{len(articles)}ç¯‡æ–‡ç« ",
                'AIå¤„ç†åå†…å®¹': processed_result['summary'],
                'å›¾ç‰‡æç¤ºè¯1': processed_result['image_prompts'][0] if processed_result['image_prompts'] else '',
                'å›¾ç‰‡æç¤ºè¯2': processed_result['image_prompts'][1] if len(processed_result['image_prompts']) > 1 else '',
                'å›¾ç‰‡æç¤ºè¯3': processed_result['image_prompts'][2] if len(processed_result['image_prompts']) > 2 else ''
            }
            
            # å°è¯•ä¿å­˜åˆ°é£ä¹¦
            success = await crawler.save_to_feishu(record_data)
            if success:
                print("âœ… é£ä¹¦å­˜å‚¨æˆåŠŸï¼")
            else:
                print("âš ï¸ é£ä¹¦å­˜å‚¨å¤±è´¥ï¼ˆå¯èƒ½æœªé…ç½®æˆ–ç½‘ç»œé—®é¢˜ï¼‰")
                
        except Exception as e:
            print(f"âš ï¸ é£ä¹¦å­˜å‚¨æµ‹è¯•å¤±è´¥: {e}")
        
        # 5. æµ‹è¯•å¾®ä¿¡æ¨é€ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
        print("\nğŸ’¬ 5. æµ‹è¯•å¾®ä¿¡æ¨é€...")
        try:
            # å‡†å¤‡æŠ¥å‘Šæ•°æ®
            report = {
                'date': datetime.now().strftime('%Y-%m-%d'),
                'summary': processed_result['summary'],
                'created_at': datetime.now().isoformat()
            }
            
            # å°è¯•å‘é€åˆ°å¾®ä¿¡
            await crawler.send_to_wechat(report)
            print("âœ… å¾®ä¿¡æ¨é€æˆåŠŸï¼")
            
        except Exception as e:
            print(f"âš ï¸ å¾®ä¿¡æ¨é€æµ‹è¯•å¤±è´¥: {e}")
        
        # 6. ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        print("\nğŸ“‹ 6. æµ‹è¯•æŠ¥å‘Š")
        print("-" * 40)
        print(f"âœ… çˆ¬è™«åŠŸèƒ½: æ­£å¸¸")
        print(f"âœ… AIå¤„ç†åŠŸèƒ½: æ­£å¸¸")
        print(f"âœ… æ—©æŠ¥ç”Ÿæˆ: æ­£å¸¸")
        print(f"âš ï¸ é£ä¹¦å­˜å‚¨: {'æ­£å¸¸' if 'success' in locals() and success else 'éœ€è¦é…ç½®'}")
        print(f"âš ï¸ å¾®ä¿¡æ¨é€: {'æ­£å¸¸' if 'report' in locals() else 'éœ€è¦é…ç½®'}")
        
        print("\nğŸ¯ ä¸‹ä¸€æ­¥å»ºè®®:")
        if 'success' not in locals() or not success:
            print("1. é…ç½®é£ä¹¦å¤šç»´è¡¨æ ¼")
        if 'report' not in locals():
            print("2. é…ç½®å¾®ä¿¡åŠ©æ‰‹")
        print("3. è®¾ç½®å®šæ—¶ä»»åŠ¡")
        print("4. éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # æ¸…ç†èµ„æº
        await crawler.cleanup()
        print("\nğŸ§¹ èµ„æºæ¸…ç†å®Œæˆ")

if __name__ == "__main__":
    asyncio.run(test_full_workflow())


