#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•çœŸå®ç½‘ç«™çˆ¬å–åŠŸèƒ½
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent))

from crawler.main import AINewsCrawler
from crawler.news_sources import NewsSources

async def test_real_crawling():
    """æµ‹è¯•çœŸå®ç½‘ç«™çˆ¬å–"""
    
    print("ğŸ•·ï¸ å¼€å§‹æµ‹è¯•çœŸå®ç½‘ç«™çˆ¬å–åŠŸèƒ½...")
    print("=" * 60)
    
    # è·å–æ–°é—»æº
    news_sources = NewsSources()
    sources = news_sources.get_sources()
    
    print(f"ğŸ“° é…ç½®çš„æ–°é—»æºæ•°é‡: {len(sources)}")
    print("\nğŸ“‹ æ–°é—»æºåˆ—è¡¨:")
    for i, source in enumerate(sources, 1):
        print(f"{i}. {source['name']} - {source['url']} ({source['category']})")
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = AINewsCrawler()
    
    try:
        # åˆå§‹åŒ–çˆ¬è™«
        await crawler.init_crawler()
        print("\nâœ… çˆ¬è™«å¼•æ“åˆå§‹åŒ–æˆåŠŸ")
        
        # é€‰æ‹©å‡ ä¸ªç½‘ç«™è¿›è¡Œæµ‹è¯•
        test_sources = [
            {"name": "36æ°ª", "url": "https://36kr.com"},
            {"name": "æœºå™¨ä¹‹å¿ƒ", "url": "https://www.jiqizhixin.com"},
            {"name": "é‡å­ä½", "url": "https://www.qbitai.com"}
        ]
        
        print(f"\nğŸ” å¼€å§‹çˆ¬å– {len(test_sources)} ä¸ªç½‘ç«™...")
        
        all_articles = []
        
        for source in test_sources:
            print(f"\nğŸ“° æ­£åœ¨çˆ¬å–: {source['name']} ({source['url']})")
            
            try:
                # æ‰§è¡Œçˆ¬å–
                result = await crawler.crawler.arun(
                    url=source['url'],
                    wait_for="networkidle",
                    delay_before_return_html=3,
                    js_code="""
                    // ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
                    await new Promise(resolve => setTimeout(resolve, 2000));
                    
                    // å°è¯•æ»šåŠ¨é¡µé¢ä»¥è§¦å‘æ‡’åŠ è½½
                    window.scrollTo(0, document.body.scrollHeight);
                    await new Promise(resolve => setTimeout(resolve, 1000));
                    window.scrollTo(0, 0);
                    """
                )
                
                if result.success:
                    print(f"âœ… {source['name']} çˆ¬å–æˆåŠŸ")
                    print(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {result.metadata.get('title', 'N/A')}")
                    print(f"ğŸ“Š å†…å®¹é•¿åº¦: {len(result.html)} å­—ç¬¦")
                    
                    # æå–æ–‡ç« ä¿¡æ¯
                    from bs4 import BeautifulSoup
                    soup = BeautifulSoup(result.html, 'html.parser')
                    
                    # æ ¹æ®ç½‘ç«™ç±»å‹æå–æ–‡ç« 
                    articles = extract_articles(soup, source['name'])
                    
                    print(f"ğŸ“° æå–åˆ° {len(articles)} ç¯‡æ–‡ç« ")
                    
                    # æ˜¾ç¤ºå‰3ç¯‡æ–‡ç« 
                    for i, article in enumerate(articles[:3], 1):
                        print(f"  {i}. {article['title'][:50]}...")
                        print(f"     æ¥æº: {article['source']}")
                        print(f"     æ‘˜è¦: {article['summary'][:80]}...")
                        print()
                    
                    all_articles.extend(articles)
                    
                else:
                    print(f"âŒ {source['name']} çˆ¬å–å¤±è´¥")
                    
            except Exception as e:
                print(f"âŒ {source['name']} çˆ¬å–å‡ºé”™: {e}")
        
        print(f"\nğŸ“Š çˆ¬å–ç»Ÿè®¡:")
        print(f"æ€»æ–‡ç« æ•°: {len(all_articles)}")
        print(f"æˆåŠŸç½‘ç«™: {len([s for s in test_sources if any(a['source'] == s['name'] for a in all_articles)])}")
        
        if all_articles:
            print(f"\nğŸ¤– ä½¿ç”¨ç«å±±æ–¹èˆŸDeepSeekå¤„ç† {len(all_articles)} ç¯‡æ–‡ç« ...")
            
            # ä½¿ç”¨å†…å®¹å¤„ç†å™¨å¤„ç†æ–‡ç« 
            from crawler.content_processor import ContentProcessor
            processor = ContentProcessor()
            result = await processor.process_articles(all_articles[:10])  # åªå¤„ç†å‰10ç¯‡
            
            print("âœ… å†…å®¹å¤„ç†å®Œæˆï¼")
            print("\nğŸ“‹ æ—©æŠ¥æ‘˜è¦:")
            print("-" * 50)
            print(result['summary'])
            
            print("\nğŸ“ˆ å‘å±•è¶‹åŠ¿:")
            print("-" * 50)
            for i, trend in enumerate(result['trends'][:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"{i}. {trend}")
            
            print("\nğŸ¨ å›¾ç‰‡æç¤ºè¯:")
            print("-" * 50)
            for i, prompt in enumerate(result['image_prompts'], 1):
                print(f"{i}. {prompt}")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # æ¸…ç†èµ„æº
        await crawler.cleanup()
        print("\nğŸ§¹ èµ„æºæ¸…ç†å®Œæˆ")

def extract_articles(soup, source_name):
    """æ ¹æ®ç½‘ç«™æå–æ–‡ç« ä¿¡æ¯"""
    articles = []
    
    try:
        if source_name == "36æ°ª":
            # 36æ°ªçš„æ–‡ç« é€‰æ‹©å™¨
            article_elements = soup.find_all('div', class_='article-item')
            for element in article_elements[:5]:  # åªå–å‰5ç¯‡
                title_elem = element.find('a', class_='article-item-title')
                if title_elem:
                    title = title_elem.get_text().strip()
                    url = title_elem.get('href', '')
                    if not url.startswith('http'):
                        url = 'https://36kr.com' + url
                    
                    summary_elem = element.find('div', class_='article-item-summary')
                    summary = summary_elem.get_text().strip() if summary_elem else title
                    
                    articles.append({
                        'title': title,
                        'summary': summary,
                        'source': source_name,
                        'url': url,
                        'publish_time': '2024-01-15T10:00:00Z'
                    })
        
        elif source_name == "æœºå™¨ä¹‹å¿ƒ":
            # æœºå™¨ä¹‹å¿ƒçš„æ–‡ç« é€‰æ‹©å™¨
            article_elements = soup.find_all('div', class_='article-item')
            for element in article_elements[:5]:
                title_elem = element.find('h3') or element.find('h2')
                if title_elem:
                    title = title_elem.get_text().strip()
                    link_elem = title_elem.find('a') or element.find('a')
                    url = link_elem.get('href', '') if link_elem else ''
                    if url and not url.startswith('http'):
                        url = 'https://www.jiqizhixin.com' + url
                    
                    summary_elem = element.find('p') or element.find('div', class_='summary')
                    summary = summary_elem.get_text().strip() if summary_elem else title
                    
                    articles.append({
                        'title': title,
                        'summary': summary,
                        'source': source_name,
                        'url': url,
                        'publish_time': '2024-01-15T10:00:00Z'
                    })
        
        elif source_name == "é‡å­ä½":
            # é‡å­ä½çš„æ–‡ç« é€‰æ‹©å™¨
            article_elements = soup.find_all('div', class_='article-item') or soup.find_all('article')
            for element in article_elements[:5]:
                title_elem = element.find('h3') or element.find('h2') or element.find('h1')
                if title_elem:
                    title = title_elem.get_text().strip()
                    link_elem = title_elem.find('a') or element.find('a')
                    url = link_elem.get('href', '') if link_elem else ''
                    if url and not url.startswith('http'):
                        url = 'https://www.qbitai.com' + url
                    
                    summary_elem = element.find('p') or element.find('div', class_='summary')
                    summary = summary_elem.get_text().strip() if summary_elem else title
                    
                    articles.append({
                        'title': title,
                        'summary': summary,
                        'source': source_name,
                        'url': url,
                        'publish_time': '2024-01-15T10:00:00Z'
                    })
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šé€‰æ‹©å™¨ï¼Œä½¿ç”¨é€šç”¨é€‰æ‹©å™¨
        if not articles:
            # é€šç”¨æ–‡ç« æå–
            for element in soup.find_all(['article', 'div'], class_=lambda x: x and 'article' in x.lower())[:5]:
                title_elem = element.find(['h1', 'h2', 'h3', 'h4'])
                if title_elem:
                    title = title_elem.get_text().strip()
                    link_elem = title_elem.find('a') or element.find('a')
                    url = link_elem.get('href', '') if link_elem else ''
                    
                    summary_elem = element.find('p')
                    summary = summary_elem.get_text().strip() if summary_elem else title
                    
                    articles.append({
                        'title': title,
                        'summary': summary,
                        'source': source_name,
                        'url': url,
                        'publish_time': '2024-01-15T10:00:00Z'
                    })
    
    except Exception as e:
        print(f"âš ï¸ æå–æ–‡ç« æ—¶å‡ºé”™: {e}")
    
    return articles

if __name__ == "__main__":
    asyncio.run(test_real_crawling())


